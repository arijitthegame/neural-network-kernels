{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNE+gF+uB8oWqTLq5d42ya1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_O4aAIUL5x7","executionInfo":{"status":"ok","timestamp":1691524078223,"user_tz":240,"elapsed":23937,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"6107d151-7efd-4b3b-cb97-75838fd42a39"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import math\n","import torch\n","from torch import nn\n","import numpy as np\n","\n","def torch_apply_along_axis(function, x, axis: int = 0):\n","    \"\"\"\n","    Torch equivalent of numpy apply along axis. This function is slow and should be avoided\n","    https://discuss.pytorch.org/t/apply-a-function-along-an-axis/130440\n","    \"\"\"\n","    return torch.stack([\n","        function(x_i) for x_i in torch.unbind(x, dim=axis)\n","    ], dim=axis)\n","\n","def input_to_rfs_torch(xw, AB_fun, ab_fun, xis, num_rfs, dim):\n","    ab_coeffs = torch_apply_along_axis(ab_fun, xis, 0)\n","    AB_coeffs = torch_apply_along_axis(AB_fun, xis, 0)\n","    torch.manual_seed(0)\n","    gs = torch.rand(size=(num_rfs, dim))\n","    renorm_gs = (ab_coeffs * gs.t()).t()\n","    dot_products = torch.einsum('ij,j->i', renorm_gs, xw)\n","    squared_xw = torch.sum(xw * xw)\n","    correction_vector = (squared_xw / 2) * ab_coeffs * ab_coeffs\n","    diff_vector = dot_products - correction_vector\n","    return (1.0 / math.sqrt(num_rfs)) * AB_coeffs * torch.exp(diff_vector)\n","\n","def input_to_rfs_torch_vectorized(xw, AB_fun, ab_fun, xis, num_rfs, dim):\n","    ab_coeffs = torch_apply_along_axis(ab_fun, xis, 0)\n","    AB_coeffs = torch_apply_along_axis(AB_fun, xis, 0)\n","    torch.manual_seed(0)\n","    gs = torch.rand(size=(num_rfs, dim))\n","    renorm_gs = (ab_coeffs * gs.t()).t()\n","    dot_products = torch.einsum('ij,jk->ik', xw, renorm_gs.t())\n","    squared_xw = torch.sum(torch.mul(xw, xw), dim=1)\n","    correction_vector = torch.outer(squared_xw / 2, torch.mul(ab_coeffs, ab_coeffs))\n","    diff_vector = dot_products - correction_vector\n","    return (1.0 / math.sqrt(num_rfs)) * AB_coeffs * torch.exp(diff_vector)\n","\n","# class mynetwork(nn.Module):\n","#     def __init__(self, w):\n","#         super().__init__()\n","#         self.w = w\n","#         self.weights = input_to_rfs_torch(self.w, A_fun, a_fun, xis, num_rfs, dim)\n","#         self.weights = nn.Parameter(self.weights)\n","#         # self.bias = nn.Parameter(torch.zeros(10))\n","\n","#     def forward(self, x):\n","#         xb = input_to_rfs_torch(x, A_fun, a_fun, xis, num_rfs, dim)\n","#         return xb @ self.weights.t()"],"metadata":{"id":"DUq1Wlq5MS-9","executionInfo":{"status":"ok","timestamp":1691524083627,"user_tz":240,"elapsed":5408,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["###################### TEST\n","# dim = 5\n","# x = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0]).float()\n","# w = torch.Tensor([5.0, 4.0, 3.0, 2.0, 1.0]).float()\n","x = torch.rand((3,256))\n","\n","# bias = torch.Tensor([0.0])\n","# groundtruth_value = torch.cos(torch.dot(x, w)+bias)\n","num_rfs = 64\n","# a_fun= lambda xi: 2.0 * math.pi * 1j * xi\n","# b_fun= lambda x: 1\n","# A_fun= lambda x: np.exp(bias)\n","# B_fun= lambda x: 1\n","a_fun = lambda x: np.sin(x)\n","b_fun = lambda x: np.cos(x)\n","A_fun = lambda x: np.sin(x)\n","B_fun = lambda x: np.cos(x)\n","\n","xis_creator = lambda x: 1.0 / (2.0 * math.pi) * (x > 0.5) - 1.0 / (2.0 * math.pi) * (x < 0.5)\n","random_tosses = torch.rand(num_rfs)\n","xis = xis_creator(random_tosses)\n","\n","class mynetwork(nn.Module):\n","    def __init__(self, inp_dim):\n","        super().__init__()\n","        self.layer1 = nn.Linear(inp_dim, inp_dim)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(inp_dim, 256)\n","        # print(self.linear2)\n","        # print(self.linear2.weight.shape)\n","        # self.w = self.linear2.weight\n","        # print(\"W IS\", w.shape)\n","        self.weights = input_to_rfs_torch_vectorized(self.linear2.weight, A_fun, a_fun, xis, num_rfs, 256)\n","        self.weights = nn.Parameter(self.weights)\n","        # self.bias = nn.Parameter(torch.zeros(10))\n","\n","    def forward(self, x):\n","        xb = input_to_rfs_torch_vectorized(x, A_fun, a_fun, xis, num_rfs, 256)\n","        return xb @ self.weights.t()"],"metadata":{"id":"vG6xBeCjQxR0","executionInfo":{"status":"ok","timestamp":1691524083628,"user_tz":240,"elapsed":19,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# testing vectorized version, before we run with Vit\n","net = mynetwork(256)\n","net(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXROgc_IRDJY","executionInfo":{"status":"ok","timestamp":1691524101147,"user_tz":240,"elapsed":131,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"d7a15372-cc51-4d67-841f-603cd2584e06"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[159.5214, 138.9836, 125.9633, 132.2636, 149.1356, 145.5340, 147.3583,\n","         151.3705, 148.8756, 149.8760, 142.3045, 151.4638, 138.9649, 149.8407,\n","         161.5794, 136.1583, 149.5099, 142.7977, 139.4826, 141.2225, 140.4835,\n","         135.7491, 156.8195, 143.1146, 146.2220, 147.1548, 138.6097, 139.5836,\n","         133.6076, 141.5825, 153.4449, 147.8783, 137.5003, 129.9559, 152.5504,\n","         145.4684, 142.0189, 142.1979, 146.1419, 143.6837, 150.1651, 147.3919,\n","         147.2217, 147.5441, 152.2361, 142.5456, 144.4930, 149.1394, 146.2385,\n","         140.4700, 153.3432, 146.4711, 141.0195, 142.8022, 147.6641, 137.6978,\n","         135.1386, 152.4140, 136.3570, 139.5619, 136.7042, 159.1589, 137.4422,\n","         152.3584, 142.0882, 149.2403, 145.5336, 148.4319, 139.8356, 134.5932,\n","         152.3497, 132.1793, 162.0865, 138.4233, 141.4406, 146.3412, 135.2060,\n","         145.7321, 138.4471, 146.4900, 152.2778, 156.8766, 147.7393, 134.8655,\n","         143.7523, 137.9660, 138.4806, 143.2652, 144.7374, 150.6664, 140.3931,\n","         145.6045, 143.9499, 147.0132, 152.3202, 137.3422, 154.5394, 161.8749,\n","         148.4725, 153.3028, 137.8123, 140.7235, 140.9844, 155.6664, 141.6002,\n","         148.8444, 146.9397, 148.1410, 135.9606, 143.7343, 134.6468, 151.6416,\n","         151.7143, 133.6558, 155.5335, 150.3241, 148.7606, 153.3575, 145.9359,\n","         139.0816, 138.1275, 144.6735, 139.0216, 137.9581, 149.6430, 150.9573,\n","         144.0970, 137.8137, 163.1881, 144.3378, 135.2306, 146.6484, 130.3203,\n","         134.6157, 137.5352, 135.4378, 144.3540, 145.5509, 155.0375, 144.2706,\n","         154.8397, 157.1985, 143.0642, 140.9319, 134.7911, 143.7301, 140.4742,\n","         143.8023, 142.0504, 143.8954, 145.5408, 151.8474, 145.2073, 156.7209,\n","         136.1047, 148.6536, 151.9398, 153.7035, 142.4056, 141.7711, 153.0579,\n","         143.5688, 133.9170, 145.7049, 153.8867, 136.6844, 141.9623, 142.8140,\n","         145.6536, 137.9055, 143.4150, 142.2370, 152.3404, 142.6083, 145.7072,\n","         135.2041, 147.2728, 137.2497, 146.7844, 143.0570, 130.9091, 148.2241,\n","         132.5890, 142.9782, 154.9359, 133.7233, 140.8363, 140.8102, 152.7677,\n","         149.1691, 157.6091, 138.6069, 143.6420, 155.3434, 146.4552, 146.3268,\n","         141.7756, 149.4629, 141.1460, 138.3219, 138.9992, 140.0105, 153.4960,\n","         134.8396, 138.6771, 131.3687, 132.3469, 157.7173, 135.1676, 146.2373,\n","         151.9868, 129.2451, 146.0894, 142.2776, 139.7188, 149.4236, 139.8362,\n","         150.1508, 149.8432, 148.4759, 137.5232, 152.9989, 145.7669, 143.6819,\n","         123.9524, 133.9367, 141.3027, 138.8490, 150.0547, 155.5288, 146.5744,\n","         156.7591, 155.2058, 148.2776, 141.2045, 144.9072, 131.6638, 153.7490,\n","         147.8141, 133.6424, 134.1607, 148.9614, 146.1269, 152.0941, 151.0135,\n","         143.9755, 137.8968, 143.2934, 159.7348, 145.0315, 148.9271, 134.9868,\n","         142.7910, 148.0365, 144.5691, 151.3055],\n","        [ 95.4970,  83.2393,  75.4770,  79.0536,  89.3416,  87.2638,  88.3396,\n","          90.7411,  89.2128,  89.9301,  85.0555,  91.1279,  83.2818,  90.0500,\n","          96.9355,  81.6586,  89.6765,  85.6423,  83.6088,  84.6839,  84.1314,\n","          81.7052,  93.9825,  86.0182,  87.9931,  88.1730,  83.1890,  83.8018,\n","          80.3519,  84.9630,  92.1906,  88.6230,  82.5398,  77.8642,  91.6998,\n","          87.2723,  84.8985,  85.2276,  87.5261,  85.8750,  89.8984,  88.4227,\n","          88.1515,  88.4122,  91.4816,  85.7740,  86.5961,  89.3220,  87.3269,\n","          84.1899,  91.8412,  87.8556,  84.7749,  85.6699,  88.5045,  82.5409,\n","          81.1200,  91.3846,  81.8935,  83.6818,  81.8944,  95.6485,  82.7593,\n","          91.1918,  85.3287,  89.3926,  87.2374,  89.0512,  83.8498,  80.6836,\n","          91.1631,  79.2980,  97.0273,  82.9392,  84.8538,  87.2903,  80.7810,\n","          87.4489,  83.0657,  87.7584,  91.2867,  94.1894,  88.8396,  80.8127,\n","          86.2223,  82.6704,  83.0108,  85.9720,  86.7881,  90.3291,  84.1620,\n","          87.3890,  86.0555,  88.1161,  91.2852,  82.3562,  92.8961,  97.0372,\n","          89.0498,  91.8393,  82.5241,  84.4665,  84.4469,  93.4859,  84.6986,\n","          89.0992,  87.8565,  88.7893,  81.5247,  86.0253,  80.6958,  90.9482,\n","          91.2064,  79.7050,  93.0231,  90.3093,  89.3146,  91.9119,  87.6045,\n","          83.3221,  82.8803,  86.9467,  83.3361,  82.8226,  89.8754,  90.3990,\n","          86.4468,  82.7694,  97.8070,  86.4150,  80.7522,  88.0879,  78.0385,\n","          80.8822,  82.4209,  81.2839,  86.6074,  86.9631,  92.6529,  86.5640,\n","          92.6034,  94.0158,  85.8529,  84.6985,  80.8271,  86.0136,  84.5015,\n","          86.2915,  84.9852,  86.2775,  87.2378,  91.3703,  86.9815,  94.0956,\n","          81.4635,  88.9951,  90.7761,  92.0831,  85.2618,  84.8920,  91.7073,\n","          85.9338,  79.9226,  87.3806,  92.0675,  81.7403,  85.3443,  85.8097,\n","          87.6435,  82.5116,  86.1683,  85.2263,  91.0118,  85.5352,  87.5037,\n","          81.2204,  88.1569,  82.0226,  87.8965,  85.5378,  78.6226,  89.0265,\n","          79.4990,  86.0432,  92.9483,  80.1813,  84.4355,  84.4726,  91.4817,\n","          89.1837,  94.5386,  83.2734,  86.0847,  93.0519,  88.0780,  87.8270,\n","          84.7906,  89.6992,  84.7361,  82.8990,  83.3480,  83.9539,  92.0516,\n","          80.9237,  83.0251,  78.8063,  79.4182,  94.7516,  80.9131,  87.6367,\n","          90.8803,  77.2656,  87.8770,  85.2944,  83.9702,  89.9580,  83.7793,\n","          90.0852,  89.7240,  88.9564,  82.4608,  91.9912,  87.5599,  86.0921,\n","          74.3324,  80.4074,  84.9322,  83.4015,  89.9697,  93.1506,  87.7578,\n","          93.8847,  93.3067,  88.7851,  84.7541,  86.6583,  78.8664,  92.0640,\n","          88.6192,  80.2244,  80.4545,  89.3838,  87.8283,  91.0757,  90.4400,\n","          86.3544,  82.4641,  86.1293,  95.9565,  87.0249,  89.2540,  81.2272,\n","          85.5115,  88.6123,  86.4721,  90.7610],\n","        [134.9772, 117.5777, 107.0839, 112.1654, 126.5787, 122.9990, 125.0451,\n","         128.8536, 125.9361, 127.2362, 120.6901, 128.9445, 117.7294, 127.3174,\n","         137.0070, 115.4054, 127.2704, 121.0268, 118.4860, 120.3350, 118.8951,\n","         115.4555, 133.1357, 121.7067, 124.1821, 124.7228, 117.6050, 118.5207,\n","         113.5455, 120.1287, 130.2527, 125.1462, 116.5035, 110.1231, 129.4011,\n","         123.3448, 120.1423, 120.8483, 124.0102, 121.5821, 127.1599, 124.8217,\n","         124.9724, 125.1726, 128.9993, 121.2383, 122.0427, 126.4654, 123.8555,\n","         118.7412, 129.8739, 124.3017, 119.8376, 120.7338, 125.3299, 116.8623,\n","         114.2928, 129.1386, 115.5390, 118.2856, 115.7273, 135.2403, 116.8667,\n","         128.7414, 120.4771, 126.4044, 123.0127, 125.6712, 118.4553, 114.2329,\n","         128.8773, 112.1459, 137.4695, 117.5434, 119.4458, 124.0354, 114.0944,\n","         123.8759, 117.6738, 124.0982, 129.1012, 133.2851, 125.6858, 114.1175,\n","         121.9048, 117.0879, 117.1036, 121.7694, 122.9309, 127.8151, 118.8939,\n","         123.2854, 121.7877, 124.6940, 129.6408, 116.3904, 131.0653, 136.9985,\n","         125.9720, 129.9190, 116.4729, 119.6423, 119.6559, 132.2583, 119.9318,\n","         126.2786, 124.3616, 125.5905, 115.4556, 121.5930, 114.2390, 128.9211,\n","         128.7355, 113.3667, 131.6471, 127.3697, 126.1413, 129.9865, 123.8637,\n","         118.2871, 117.4787, 122.7287, 118.0216, 117.0302, 127.0959, 127.9868,\n","         122.2336, 116.6816, 138.3995, 122.1768, 114.4786, 124.3263, 110.2848,\n","         114.1985, 116.6801, 114.7251, 122.3833, 123.0739, 131.0374, 122.4208,\n","         131.1116, 133.3846, 121.6329, 119.8344, 114.3038, 121.5273, 119.2777,\n","         121.8254, 120.0200, 121.7188, 123.5232, 128.8653, 122.9546, 132.9015,\n","         115.2121, 126.0609, 128.4259, 130.4118, 121.0441, 120.1563, 129.8294,\n","         121.6939, 113.1357, 123.2328, 130.0697, 115.7453, 120.8551, 121.0639,\n","         123.7350, 116.8201, 121.5883, 120.7200, 129.0537, 120.8734, 123.5881,\n","         114.7258, 124.6756, 116.1571, 124.4432, 121.0853, 111.2836, 125.8390,\n","         112.3962, 121.8009, 131.4244, 113.5654, 119.5832, 119.8875, 129.6113,\n","         126.2418, 133.5985, 117.6589, 121.5945, 131.6591, 124.4048, 124.0801,\n","         120.0611, 126.8158, 119.7317, 117.1999, 117.7485, 118.9085, 130.1095,\n","         114.5231, 117.7683, 111.4094, 112.3987, 134.1067, 114.4520, 124.0645,\n","         128.4380, 109.1327, 124.2224, 120.8080, 118.4220, 127.2949, 118.7779,\n","         127.0361, 127.0862, 125.8353, 116.6069, 130.3165, 123.6606, 121.6092,\n","         105.0620, 113.4539, 119.9096, 117.9800, 127.4954, 131.2901, 123.9822,\n","         132.6257, 131.7726, 125.5206, 119.7659, 123.1788, 111.5864, 130.4582,\n","         125.3147, 113.8525, 113.5208, 126.4812, 123.9089, 129.3571, 127.9968,\n","         122.0313, 116.7486, 121.8255, 135.5361, 122.9092, 126.4600, 114.8428,\n","         121.0158, 125.2739, 122.7441, 128.4528]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["for n, p in net.named_parameters():\n","  print(n)\n","  if 'weights' in n:\n","    p.requires_grad = True\n","  else:\n","    p.requires_grad = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgOCWn8bTxRH","executionInfo":{"status":"ok","timestamp":1691524104816,"user_tz":240,"elapsed":129,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"858e0cf2-e3ea-47f2-e777-f3cc4ff50ade"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["weights\n","layer1.weight\n","layer1.bias\n","linear2.weight\n","linear2.bias\n"]}]},{"cell_type":"code","source":["net.train()\n","optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n","# real stupid test\n","for i in range(50):\n","    optimizer.zero_grad()\n","    l = net(x).mean()\n","    if i%5==0:\n","      print(l)\n","    l.backward() #quite slow\n","    optimizer.step()\n","# loss indeed goes down"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDa5QvXqUNGo","executionInfo":{"status":"ok","timestamp":1691524108310,"user_tz":240,"elapsed":305,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"95f74846-2b67-42a3-f158-3975175187bb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(117.9060, grad_fn=<MeanBackward0>)\n","tensor(114.9361, grad_fn=<MeanBackward0>)\n","tensor(111.9663, grad_fn=<MeanBackward0>)\n","tensor(108.9964, grad_fn=<MeanBackward0>)\n","tensor(106.0266, grad_fn=<MeanBackward0>)\n","tensor(103.0567, grad_fn=<MeanBackward0>)\n","tensor(100.0869, grad_fn=<MeanBackward0>)\n","tensor(97.1170, grad_fn=<MeanBackward0>)\n","tensor(94.1472, grad_fn=<MeanBackward0>)\n","tensor(91.1773, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"code","source":["# # test vectorized version.\n","# # the output does match the non-vectorized version\n","# x_vec = torch.Tensor([[1.0, 2.0, 3.0, 4.0, 5.0], [5.0, 4.0, 3.0, 2.0, 1.0]]).float()\n","# x_rfs_vec = input_to_rfs_torch_vectorized(x_vec, A_fun, a_fun, xis, num_rfs, dim)\n","# print('vectorized output: ', x_rfs_vec)\n","# x_rfs = input_to_rfs_torch(x, A_fun, a_fun, xis, num_rfs, dim)\n","# print('non-vectorized output: ', x_rfs)\n","# x_2 = torch.Tensor([5.0, 4.0, 3.0, 2.0, 1.0]).float()\n","# x_rfs_2 = input_to_rfs_torch(x_2, A_fun, a_fun, xis, num_rfs, dim)\n","# print('non-vectorized output (2nd vector): ', x_rfs_2)"],"metadata":{"id":"6mcBTyiXZ-Em","executionInfo":{"status":"aborted","timestamp":1691524084115,"user_tz":240,"elapsed":10,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install transformers datasets evaluate wandb accelerate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYDJe-yvV3Bx","executionInfo":{"status":"ok","timestamp":1691524122356,"user_tz":240,"elapsed":8176,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"874669ef-f123-443b-bf5a-6ddd89af12c5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.8)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.32)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.29.2)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"code","source":["from collections import OrderedDict\n","import collections.abc\n","import math\n","from typing import Dict, List, Optional, Set, Tuple, Union\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","\n","from transformers.activations import ACT2FN\n","from transformers.modeling_outputs import (\n","    BaseModelOutput,\n","    BaseModelOutputWithPooling,\n","    ImageClassifierOutput,\n","    MaskedImageModelingOutput,\n",")\n","from transformers.modeling_utils import PreTrainedModel\n","from transformers.pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n","from transformers.utils import (\n","    add_code_sample_docstrings,\n","    add_start_docstrings,\n","    add_start_docstrings_to_model_forward,\n","    logging,\n","    replace_return_docstrings,\n",")\n","from transformers import ViTConfig, ViTPreTrainedModel, ViTModel\n","from transformers.models.vit.modeling_vit import ViTPooler\n","# from nnk import *\n","\n","logger = logging.get_logger(__name__)\n","\n","# General docstring\n","_CONFIG_FOR_DOC = \"ViTConfig\"\n","\n","# Base docstring\n","_CHECKPOINT_FOR_DOC = \"google/vit-base-patch16-224-in21k\"\n","_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n","\n","# Image classification docstring\n","_IMAGE_CLASS_CHECKPOINT = \"google/vit-base-patch16-224\"\n","_IMAGE_CLASS_EXPECTED_OUTPUT = \"Egyptian cat\"\n","\n","\n","VIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n","    \"google/vit-base-patch16-224\",\n","    # See all ViT models at https://huggingface.co/models?filter=vit\n","]\n","\n","VIT_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n","            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`ViTImageProcessor.__call__`]\n","            for details.\n","\n","        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n","            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n","\n","            - 1 indicates the head is **not masked**,\n","            - 0 indicates the head is **masked**.\n","\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        interpolate_pos_encoding (`bool`, *optional*):\n","            Whether to interpolate the pre-trained position encodings.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\""],"metadata":{"id":"ck9B2SxjV3FR","executionInfo":{"status":"ok","timestamp":1691524135361,"user_tz":240,"elapsed":13013,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class Identity(nn.Module):\n","    def __init__(self):\n","        super(Identity, self).__init__()\n","\n","    def forward(self, x):\n","        return x\n","\n","class LinearViTForImageClassification(ViTPreTrainedModel):\n","    def __init__(self, config: ViTConfig, A_fun: callable, a_fun: callable, xis: callable, num_rfs: int) -> None:\n","        super().__init__(config, A_fun, a_fun, xis, num_rfs)\n","\n","        self.num_labels = config.num_labels\n","        self.A_fun = A_fun\n","        self.a_fun = a_fun\n","        self.xis = xis\n","        self.num_rfs = num_rfs\n","        # self.vit = ViTModel(config, add_pooling_layer=False) #the og image classification model do not use the pooling layer\n","        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","        self.vit.pooler = Identity()\n","        self.pooler = ViTPooler(config) #add pooling layer\n","        # self.pooler = self.pooler.load_state_dict(dict1, strict=False)\n","\n","        # Classifier head\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n","\n","        self.post_init()\n","        # Initialize weights and apply final processing\n","        # get weights and bias to linearize\n","        self.w = self.pooler.dense.weight\n","        # the bias in the pooler layer is the 0 vector so going to ignore the first pass\n","        self.output_rfs = input_to_rfs_torch_vectorized(self.w, A_fun, a_fun, xis, num_rfs, self.w.shape[1])\n","        # TO CHECK: Might be issues with some gradient hooks\n","        self.output_rfs = nn.Parameter(self.output_rfs)\n","        # linearize the pooler layer\n","\n","\n","    @add_start_docstrings_to_model_forward(VIT_INPUTS_DOCSTRING)\n","    @add_code_sample_docstrings(\n","        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n","        output_type=ImageClassifierOutput,\n","        config_class=_CONFIG_FOR_DOC,\n","        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n","    )\n","    # TODO : Precompute the x_rfs\n","    def forward(\n","        self,\n","        pixel_values: Optional[torch.Tensor] = None,\n","        head_mask: Optional[torch.Tensor] = None,\n","        labels: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        interpolate_pos_encoding: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[tuple, ImageClassifierOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.vit(\n","            pixel_values,\n","            head_mask=head_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            interpolate_pos_encoding=interpolate_pos_encoding,\n","            return_dict=return_dict,\n","        )\n","        print(outputs)\n","        first_token_tensor = outputs['last_hidden_state'][:, 0]\n","        x_rfs = input_to_rfs_torch_vectorized(first_token_tensor, self.A_fun, self.a_fun, self.xis, self.num_rfs, first_token_tensor.shape[1])\n","        sequence_output = x_rfs @ self.output_rfs.t()\n","        # compute the linearized pooling layer\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            # move labels to correct device to enable model parallelism\n","            labels = labels.to(logits.device)\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return ImageClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"],"metadata":{"id":"AvAhq_lNV3HR","executionInfo":{"status":"ok","timestamp":1691524135363,"user_tz":240,"elapsed":9,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n","model = LinearViTForImageClassification(config,A_fun=A_fun, a_fun=a_fun, xis=xis, num_rfs=64)"],"metadata":{"id":"QlQR_fXrWh1R","executionInfo":{"status":"ok","timestamp":1691524140489,"user_tz":240,"elapsed":5134,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# model1 = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","# with torch.no_grad():\n","#     model.pooler.dense.weight.copy_(model1.pooler.dense.weight)"],"metadata":{"id":"xwO4uSYahQ_f","executionInfo":{"status":"ok","timestamp":1691524140490,"user_tz":240,"elapsed":17,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["dict1 = torch.load('pooler_weights.pkl')\n","with torch.no_grad():\n","    model.pooler.dense.weight.copy_(dict1['pooler.dense.weight'])"],"metadata":{"id":"rVzAcyNarSB1","executionInfo":{"status":"ok","timestamp":1691524140490,"user_tz":240,"elapsed":15,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQv6oC83dzeQ","executionInfo":{"status":"ok","timestamp":1691524140491,"user_tz":240,"elapsed":15,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"6cd5226b-06ca-4135-eb66-1adaa96882de"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearViTForImageClassification(\n","  (vit): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (pooler): Identity()\n","  )\n","  (pooler): ViTPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["model.pooler.dense.weight == dict1['pooler.dense.weight']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSmfoFUahXq_","executionInfo":{"status":"ok","timestamp":1691524140491,"user_tz":240,"elapsed":14,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"fc367ab6-129e-41cc-f089-a6d6c3fcc2c5"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        ...,\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from datasets import load_dataset, load_metric\n","from transformers import ViTImageProcessor\n","dataset = load_dataset(\"huggingface/cats-image\")\n","image = dataset[\"test\"][\"image\"][0]\n","image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","inputs = image_processor(image, return_tensors=\"pt\")\n"],"metadata":{"id":"0ZUEC1WzhlSf","executionInfo":{"status":"ok","timestamp":1691524144786,"user_tz":240,"elapsed":4302,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model(**inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQYR_aHMhljd","executionInfo":{"status":"ok","timestamp":1691524146852,"user_tz":240,"elapsed":2071,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}},"outputId":"863d9a44-3ae2-4dea-8de3-ea713b16abe7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]],\n","       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"]},{"output_type":"execute_result","data":{"text/plain":["ImageClassifierOutput(loss=None, logits=tensor([[-0.0259,  0.0105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["for n, p in model.named_parameters():\n","    if ('output_rfs'in n) or ('classifier' in n):\n","        p.requires_grad = True\n","    else :\n","        p.requires_grad = False"],"metadata":{"id":"-cTLYb-rkldg","executionInfo":{"status":"ok","timestamp":1691524146853,"user_tz":240,"elapsed":5,"user":{"displayName":"yunfan zhao","userId":"09973238562806198500"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["model.train()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","# real stupid test\n","for i in range(10):\n","    optimizer.zero_grad()\n","    l = model(inputs['pixel_values'], labels=torch.tensor([1]).long()).loss\n","    if i%1 == 0:\n","      print(l)\n","    l.backward() #quite slow\n","    optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsMtFkwdk_HP","outputId":"50012b40-9291-4d35-ca85-0c37621590d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), pooler_output=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), hidden_states=None, attentions=None)\n","tensor(0.6751, grad_fn=<NllLossBackward0>)\n","BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), pooler_output=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), hidden_states=None, attentions=None)\n","tensor(0.6708, grad_fn=<NllLossBackward0>)\n","BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), pooler_output=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), hidden_states=None, attentions=None)\n","tensor(0.6666, grad_fn=<NllLossBackward0>)\n","BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), pooler_output=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), hidden_states=None, attentions=None)\n","tensor(0.6623, grad_fn=<NllLossBackward0>)\n","BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), pooler_output=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), hidden_states=None, attentions=None)\n","tensor(0.6581, grad_fn=<NllLossBackward0>)\n","BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), pooler_output=tensor([[[ 0.1559,  0.0914,  0.1518,  ..., -0.3180, -0.0859, -0.0903],\n","         [-0.2254,  0.0864,  0.4752,  ..., -0.1781,  0.1726,  0.1334],\n","         [ 0.0444,  0.0677,  0.4199,  ..., -0.2576,  0.1191,  0.0130],\n","         ...,\n","         [-0.0153, -0.0396,  0.1684,  ..., -0.1672,  0.1869,  0.1025],\n","         [ 0.0249, -0.0382,  0.2046,  ...,  0.0517,  0.1489,  0.1320],\n","         [-0.1748, -0.0254,  0.2523,  ..., -0.1474,  0.1627,  0.1325]]]), hidden_states=None, attentions=None)\n","tensor(0.6539, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_KsbcDHAam7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fq_7eXJ2bvUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k9O0MfBvjsJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the pooler weights.\n","# pytorch_model.bin comes from:\n","# https://huggingface.co/google/vit-base-patch16-224-in21k/tree/main\n","# need to throw the bin file into the left side panel\n","\n","# weights = torch.load('/content/pytorch_model.bin')\n","\n","# dict1 = OrderedDict()\n","# dict1['pooler.dense.weight'] = weights['pooler.dense.weight']\n","# dict1['pooler.dense.bias'] = weights['pooler.dense.bias']\n","\n","# torch.save(dict1, '/content/pooler_weights.pkl')"],"metadata":{"id":"6LjTYPGmYOnw"},"execution_count":null,"outputs":[]}]}