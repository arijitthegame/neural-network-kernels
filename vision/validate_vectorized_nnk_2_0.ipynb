{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "f_hGEicZ1_3x"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### validate non-vectorized version ###\n",
        "# in the non-vectorized version, input arguments align with old code. so we are back compatible"
      ],
      "metadata": {
        "id": "Kxo8KqQgSKDz"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplary instantiations of functions: a_fun, b_fun, A_fun, B_fun.\n",
        "# The functions need to satisfy conditions:\n",
        "#\n",
        "# a_fun(x) * b_fun(x) = 2.0 * math.pi * 1j * x\n",
        "# A_fun(x, b) * B_fun(x, b) = np.exp(2.0 * math.pi * 1j * x * b)\n",
        "\n",
        "a_fun = lambda x: 2.0 * math.pi * 1j * x\n",
        "b_fun = lambda x: 1\n",
        "A_fun = lambda x, b: np.exp(2.0 * math.pi * 1j * x * b)\n",
        "B_fun = lambda x, b: 1\n"
      ],
      "metadata": {
        "id": "oEBSMvvx2I1H"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main mapping producing <num_rand_features>-dimensional random feature vector\n",
        "# and used to approximate: E_{xi~P} [exp(2*pi*i*xi*(x^t*w+bias_term))].\n",
        "# Can be used to approximate:\n",
        "# f(x^t*w+b) = \\int_{R} [F(xi)exp(2*pi*i*xi*(x^t*w+bias_term)) dxi], where\n",
        "# F is a Fourier Transform of f.\n",
        "#\n",
        "# xw: input dim-dimensional vector (either x or w)\n",
        "# bias_term: the mapping is used to approximate: f(x^tw + bias_term)\n",
        "# xis: <num_rand_features>-dimensional vector of scalars\n",
        "#      (depends on the form of the approximated f)\n",
        "# large_fun: A_fun (if input vector is x) or B_fun (if input vector is w)\n",
        "# small_fun: a_fun (if input vector is x) or B_fun (if input vector is w)\n",
        "# num_rand_features: number of random features used\n",
        "# M: hyperparameter of the algorithm (positive scalar)\n",
        "# dim: dimensionality of the input vector\n",
        "# seed: seed used contruct Gaussian projections\n",
        "\n",
        "\n",
        "def phi_mapping(xw, bias_term, xis, large_fun, small_fun, num_rand_features, M, dim, seed=0):\n",
        "  small_coeffs = np.apply_along_axis(small_fun, 0, xis)\n",
        "  large_bias_fun = lambda x: large_fun(x, bias_term)\n",
        "  large_coeffs = np.apply_along_axis(large_bias_fun, 0, xis)\n",
        "  # np.random.seed(seed)\n",
        "  # gs = np.random.normal(size=(num_rand_features, dim))\n",
        "  # using torch.rand to match the random numbers in the vectorized version (np.random and torch.rand give different numbers for the same seed)\n",
        "  torch.manual_seed(seed)\n",
        "  gs = torch.normal(mean=0, std=1, size=(num_rand_features, dim))\n",
        "  gs = gs.numpy()\n",
        "  renorm_gs = np.transpose(np.sqrt(1+4.0 * M) * small_coeffs * np.transpose(gs))\n",
        "  dot_products = np.einsum('ij,j->i', renorm_gs, xw)\n",
        "  squared_xw = np.sum(xw * xw)\n",
        "  correction_vector = (squared_xw / 2) * small_coeffs * small_coeffs\n",
        "  correction_vector += M * np.linalg.norm(gs, axis=-1) * np.linalg.norm(gs, axis=-1)\n",
        "  diff_vector = dot_products - correction_vector\n",
        "\n",
        "  return (1.0 / np.sqrt(num_rand_features)) * large_coeffs * np.exp(diff_vector)\n"
      ],
      "metadata": {
        "id": "_qaFQ5L65sFv"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def phi_mapping(xw, bias_term, xis, large_fun, small_fun, num_rand_features, M, dim, seed=0):\n",
        "def input_to_rfs_torch(xw, AB_fun, ab_fun, xis, num_rfs, dim, device,\n",
        "                                  seed=0, normalize=False, normalization_constant=None,\n",
        "                                  orthogonal=False, proj_matrix=None, bias_term=0, M=1):\n",
        "  small_coeffs = np.apply_along_axis(ab_fun, 0, xis)\n",
        "  large_bias_fun = lambda x: AB_fun(x, bias_term)\n",
        "  large_coeffs = np.apply_along_axis(large_bias_fun, 0, xis)\n",
        "  # np.random.seed(seed)\n",
        "  # gs = np.random.normal(size=(num_rfs, dim))\n",
        "  # using torch.rand to match the random numbers in the vectorized version (np.random and torch.rand give different numbers for the same seed)\n",
        "  torch.manual_seed(seed)\n",
        "  gs = torch.normal(mean=0, std=1, size=(num_rand_features, dim))\n",
        "  gs = gs.numpy()\n",
        "  renorm_gs = np.transpose(np.sqrt(1+4.0 * M) * small_coeffs * np.transpose(gs))\n",
        "  dot_products = np.einsum('ij,j->i', renorm_gs, xw)\n",
        "  squared_xw = np.sum(xw * xw)\n",
        "  correction_vector = (squared_xw / 2) * small_coeffs * small_coeffs\n",
        "  correction_vector += M * np.linalg.norm(gs, axis=-1) * np.linalg.norm(gs, axis=-1)\n",
        "  diff_vector = dot_products - correction_vector\n",
        "\n",
        "  return (1.0 / np.sqrt(num_rfs)) * large_coeffs * np.exp(diff_vector)\n"
      ],
      "metadata": {
        "id": "AzwAfjngI02x"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example for: cos(x^Tw + bias).\n",
        "dim = 3\n",
        "L = 4\n",
        "bias = 1.0\n",
        "X = np.random.uniform(size=(L, dim))\n",
        "W = np.random.uniform(size=(L, dim))\n",
        "G = np.einsum('ij,kj->ik', X, W)\n",
        "\n",
        "groundtruth_values = np.cos(G + bias)\n",
        "print(\"Groundtruth:\")\n",
        "print(groundtruth_values)\n",
        "\n",
        "# Hyperparameters of the algorithm.\n",
        "M = 1.0\n",
        "num_rand_features = 100000\n",
        "seed = 0\n",
        "\n",
        "# xis scalars for the cosine mapping f.\n",
        "xis_plus = np.ones(num_rand_features) * (1.0 / (2.0 * math.pi))\n",
        "xis_minus = - xis_plus\n",
        "\n",
        "# Computing random features.\n",
        "results = []\n",
        "for i in range(L):\n",
        "  first_rfv_plus = phi_mapping(X[i], bias, xis_plus, A_fun, a_fun, num_rand_features, M, dim, seed)\n",
        "  first_rfv_minus = phi_mapping(X[i], bias, xis_minus, A_fun, a_fun, num_rand_features, M, dim, seed)\n",
        "  first_rfv = (1.0 / np.sqrt(2.0)) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((first_rfv_plus, first_rfv_minus), axis=-1)\n",
        "  for j in range(L):\n",
        "    second_rfv_plus = phi_mapping(W[j], bias, xis_plus, B_fun, b_fun, num_rand_features, M, dim, seed)\n",
        "    second_rfv_minus = phi_mapping(W[j], bias, xis_minus, B_fun, b_fun, num_rand_features, M, dim, seed)\n",
        "    # Final rfvs for the cosine mapping.\n",
        "    second_rfv = (1.0 / np.sqrt(2.0)) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((second_rfv_plus, second_rfv_minus), axis=-1)\n",
        "\n",
        "    approximate_value = 0.5 * np.power(1.0 + M, dim / 2.0) * (np.dot(first_rfv_plus, second_rfv_plus) + np.dot(first_rfv_minus, second_rfv_minus))\n",
        "    approximate_value = np.dot(first_rfv, second_rfv)\n",
        "    results.append(np.real(approximate_value))\n",
        "print(\"Approximate\")\n",
        "print(np.reshape(np.array(results), (L, L)))\n",
        "\n",
        "# validate back compatible code\n",
        "results_2 = []\n",
        "for i in range(L):\n",
        "  first_rfv_plus = input_to_rfs_torch(X[i], A_fun, a_fun, xis_plus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "  first_rfv_minus = input_to_rfs_torch(X[i], A_fun, a_fun, xis_minus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "  first_rfv = (1.0 / np.sqrt(2.0)) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((first_rfv_plus, first_rfv_minus), axis=-1)\n",
        "  for j in range(L):\n",
        "    second_rfv_plus = input_to_rfs_torch(W[j], B_fun, b_fun, xis_plus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "    second_rfv_minus = input_to_rfs_torch(W[j], B_fun, b_fun, xis_minus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "\n",
        "    # Final rfvs for the cosine mapping.\n",
        "    second_rfv = (1.0 / np.sqrt(2.0)) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((second_rfv_plus, second_rfv_minus), axis=-1)\n",
        "\n",
        "    approximate_value = 0.5 * np.power(1.0 + M, dim / 2.0) * (np.dot(first_rfv_plus, second_rfv_plus) + np.dot(first_rfv_minus, second_rfv_minus))\n",
        "    approximate_value = np.dot(first_rfv, second_rfv)\n",
        "    results_2.append(np.real(approximate_value))\n",
        "print(\"Approximate\")\n",
        "print(np.reshape(np.array(results_2), (L, L)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wQBXIQEi-MD",
        "outputId": "8d14d78a-3f2e-4543-89a0-35b98612dd8d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groundtruth:\n",
            "[[-0.09871976 -0.46026593 -0.32263711  0.18555275]\n",
            " [ 0.4152287   0.13229234  0.1517042   0.00663355]\n",
            " [-0.216538   -0.8542541  -0.68533179 -0.47266385]\n",
            " [ 0.06901962 -0.68062977 -0.34661228 -0.36555236]]\n",
            "Approximate\n",
            "[[-0.10751362 -0.46808341 -0.3317336   0.17797384]\n",
            " [ 0.41773888  0.13403115  0.15296017  0.00363442]\n",
            " [-0.23466393 -0.86620414 -0.70998969 -0.48148429]\n",
            " [ 0.0671927  -0.6823076  -0.3509625  -0.36831847]]\n",
            "Approximate\n",
            "[[-0.10751362 -0.46808341 -0.3317336   0.17797384]\n",
            " [ 0.41773888  0.13403115  0.15296017  0.00363442]\n",
            " [-0.23466393 -0.86620414 -0.70998969 -0.48148429]\n",
            " [ 0.0671927  -0.6823076  -0.3509625  -0.36831847]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example for: sin(x^Tw + bias).\n",
        "dim = 3\n",
        "L = 4\n",
        "bias = 0.5\n",
        "X = np.random.uniform(size=(L, dim))\n",
        "W = np.random.uniform(size=(L, dim))\n",
        "G = np.einsum('ij,kj->ik', X, W)\n",
        "\n",
        "groundtruth_values = np.sin(G + bias)\n",
        "print(\"Groundtruth:\")\n",
        "print(groundtruth_values)\n",
        "\n",
        "# Hyperparameters of the algorithm.\n",
        "M = 1.0\n",
        "num_rand_features = 100000\n",
        "seed = 0\n",
        "\n",
        "# xis scalars for the cosine mapping f.\n",
        "xis_plus = np.ones(num_rand_features) * (1.0 / (2.0 * math.pi))\n",
        "xis_minus = - xis_plus\n",
        "\n",
        "# Computing random features.\n",
        "results = []\n",
        "for i in range(L):\n",
        "  for j in range(L):\n",
        "    first_rfv_plus = phi_mapping(X[i], bias, xis_plus, A_fun, a_fun, num_rand_features, M, dim, seed)\n",
        "    second_rfv_plus = phi_mapping(W[j], bias, xis_plus, B_fun, b_fun, num_rand_features, M, dim, seed)\n",
        "    first_rfv_minus = phi_mapping(X[i], bias, xis_minus, A_fun, a_fun, num_rand_features, M, dim, seed)\n",
        "    second_rfv_minus = phi_mapping(W[j], bias, xis_minus, B_fun, b_fun, num_rand_features, M, dim, seed)\n",
        "\n",
        "    # Final rfvs for the cosine mapping.\n",
        "    first_rfv = (1.0 / 2.0) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((first_rfv_plus, first_rfv_minus), axis=-1)\n",
        "    second_rfv = (1.0 / 1.0j) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((second_rfv_plus, -second_rfv_minus), axis=-1)\n",
        "\n",
        "    approximate_value = 0.5 * np.power(1.0 + M, dim / 2.0) * (np.dot(first_rfv_plus, second_rfv_plus) + np.dot(first_rfv_minus, second_rfv_minus))\n",
        "    approximate_value = np.dot(first_rfv, second_rfv)\n",
        "    results.append(np.real(approximate_value))\n",
        "print(\"Approximate:\")\n",
        "print(np.reshape(np.array(results), (L, L)))\n",
        "\n",
        "# validate back compatible code\n",
        "results_2 = []\n",
        "for i in range(L):\n",
        "  first_rfv_plus = input_to_rfs_torch(X[i], A_fun, a_fun, xis_plus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "  first_rfv_minus = input_to_rfs_torch(X[i], A_fun, a_fun, xis_minus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "  first_rfv = (1.0 / 2.0) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((first_rfv_plus, first_rfv_minus), axis=-1)\n",
        "  for j in range(L):\n",
        "    second_rfv_plus = input_to_rfs_torch(W[j], B_fun, b_fun, xis_plus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "    second_rfv_minus = input_to_rfs_torch(W[j], B_fun, b_fun, xis_minus, num_rand_features, dim, device='cpu', seed=seed, bias_term=bias, M=M)\n",
        "\n",
        "    # Final rfvs for the cosine mapping.\n",
        "    second_rfv = (1.0 / 1.0j) * np.power(1.0 + 4.0 * M, dim / 4.0) * np.concatenate((second_rfv_plus, -second_rfv_minus), axis=-1)\n",
        "\n",
        "    approximate_value = 0.5 * np.power(1.0 + M, dim / 2.0) * (np.dot(first_rfv_plus, second_rfv_plus) + np.dot(first_rfv_minus, second_rfv_minus))\n",
        "    approximate_value = np.dot(first_rfv, second_rfv)\n",
        "    results_2.append(np.real(approximate_value))\n",
        "print(\"Approximate\")\n",
        "print(np.reshape(np.array(results_2), (L, L)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWoURyzVENd9",
        "outputId": "cf247f57-4de2-4ad9-aa54-32ce6a81c1e6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groundtruth:\n",
            "[[0.99336994 0.99537018 0.97433318 0.92207096]\n",
            " [0.88893583 0.93166626 0.83373491 0.81176232]\n",
            " [0.90103314 0.80373569 0.887045   0.76288439]\n",
            " [0.75886593 0.84960487 0.6876303  0.71699613]]\n",
            "Approximate:\n",
            "[[0.99824607 1.01340828 0.97738155 0.929027  ]\n",
            " [0.89181886 0.94139565 0.83576861 0.81486762]\n",
            " [0.90064885 0.80887199 0.88611532 0.76211318]\n",
            " [0.7575847  0.85446197 0.68558617 0.71637163]]\n",
            "Approximate\n",
            "[[0.99824607 1.01340828 0.97738155 0.929027  ]\n",
            " [0.89181886 0.94139565 0.83576861 0.81486762]\n",
            " [0.90064885 0.80887199 0.88611532 0.76211318]\n",
            " [0.7575847  0.85446197 0.68558617 0.71637163]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # RELU variant:\n",
        "\n",
        "# def phi_relu_mapping(xw, num_rand_features, dim, seed=0):\n",
        "#   np.random.seed(seed)\n",
        "#   gs = np.random.normal(size=(num_rand_features, dim))\n",
        "#   dot_products = np.einsum('ij,j->i', gs, xw)\n",
        "#   return (1.0 / np.sqrt(num_rand_features)) * np.maximum(dot_products, np.zeros(num_rand_features))"
      ],
      "metadata": {
        "id": "Yzf6bqBOLHme"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### validate vectorized version ###"
      ],
      "metadata": {
        "id": "G_nSujw5-9gq"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/NNK/vectorized_validation/new_nnk')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92tbm_mknXin",
        "outputId": "4f21bc56-1ff0-45aa-df46-e446aeaa81a5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch import nn\n",
        "from performer_attention import gaussian_orthogonal_random_matrix\n",
        "\n",
        "def torch_apply_along_axis(function, x, axis: int = 0, b=None):\n",
        "    \"\"\"\n",
        "    Torch equivalent of numpy apply along axis. This function is slow and should be avoided\n",
        "    https://discuss.pytorch.org/t/apply-a-function-along-an-axis/130440\n",
        "    \"\"\"\n",
        "    if b is None:\n",
        "      return torch.stack([\n",
        "          function(x_i) for x_i in torch.unbind(x, dim=axis)\n",
        "      ], dim=axis)\n",
        "    else:\n",
        "      return torch.stack([\n",
        "          function(x_i, b) for x_i in torch.unbind(x, dim=axis)\n",
        "      ], dim=axis)\n",
        "\n",
        "# A_fun = lambda x, b: np.exp(2.0 * math.pi * 1j * x * b)\n",
        "\n",
        "def input_to_rfs_torch_vectorized(xw, AB_fun, ab_fun, xis, num_rfs, dim, device,\n",
        "                                  seed=0, normalize=False, normalization_constant=None,\n",
        "                                  orthogonal=False, proj_matrix=None, bias_term=0, M=1):\n",
        "    if normalize :\n",
        "      if normalization_constant is None :\n",
        "        xw = torch.nn.functional.normalize(xw)\n",
        "      else :\n",
        "        xw = normalization_constant*torch.nn.functional.normalize(xw)\n",
        "\n",
        "    ab_coeffs = torch_apply_along_axis(ab_fun, xis, 0)\n",
        "    AB_coeffs = torch_apply_along_axis(AB_fun, xis, 0, bias_term)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if proj_matrix is None:\n",
        "      if device == 'cpu':\n",
        "        if orthogonal is False :\n",
        "          gs = torch.normal(mean=0, std=1, size=(num_rand_features, dim))\n",
        "        else :\n",
        "          gs = gaussian_orthogonal_random_matrix(num_rfs, dim, scaling = 0, device = 'cpu')\n",
        "      else :\n",
        "        if orthogonal is False :\n",
        "          gs = torch.normal(mean=0, std=1, size=(num_rand_features, dim))\n",
        "        else :\n",
        "          gs = gaussian_orthogonal_random_matrix(num_rfs, dim, scaling = 0, device = 'cuda')\n",
        "    else :\n",
        "      if device == 'cpu':\n",
        "        gs = proj_matrix\n",
        "      else :\n",
        "         gs = proj_matrix.cuda()\n",
        "    renorm_gs = (torch.sqrt(torch.Tensor([1+4.0 * M])) * ab_coeffs * gs.t()).t()\n",
        "    if len(xw.shape) == 2 :\n",
        "      # renorm_gs has complex numbers, and torch.eimsum needs both inputs to be complex doubles\n",
        "      dot_products = torch.einsum('ij,jk->ik', xw.type(torch.complex128), renorm_gs.t())\n",
        "    elif len(xw.shape) == 3:\n",
        "      dot_products = torch.einsum('bij,jk->bik', xw.type(torch.complex128), renorm_gs.t())\n",
        "    else :\n",
        "      raise ValueError(\"Unsuported Tensor shape\")\n",
        "    squared_xw = torch.sum(torch.mul(xw, xw), dim=-1) #do not keepdims here\n",
        "    if len(squared_xw.shape) == 1 :\n",
        "      correction_vector = torch.outer(squared_xw / 2, torch.mul(ab_coeffs, ab_coeffs))\n",
        "    elif len(squared_xw.shape) == 2 :\n",
        "      correction_vector = torch.einsum('pq, r->pqr', squared_xw, torch.mul(ab_coeffs, ab_coeffs))\n",
        "    else :\n",
        "      raise ValueError(\"Unsupported tensor shape of xw\")\n",
        "    correction_vector += M * torch.linalg.norm(gs, axis=-1) * torch.linalg.norm(gs, axis=-1)\n",
        "    # check the shape of dot_products and correctino_vector\n",
        "    diff_vector = dot_products - correction_vector\n",
        "    return (1.0 / math.sqrt(num_rfs)) * AB_coeffs * torch.exp(diff_vector)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bGtWQm4e-9u5"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test vectorized version\n",
        "np.random.seed(0)\n",
        "x_vec = np.random.uniform(size=(L, dim))\n",
        "x_rfs_concatenated = []\n",
        "for i in range(len(x_vec)):\n",
        "  x_rfs = input_to_rfs_torch(x_vec[i], A_fun, a_fun, xis_plus, num_rand_features, dim, device='cpu', bias_term=bias)\n",
        "  x_rfs_concatenated.append(x_rfs)\n",
        "x_rfs_concatenated = np.array(x_rfs_concatenated)\n",
        "print(x_rfs_concatenated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZXybdfqNLM7",
        "outputId": "d0e6f7ce-fe93-449e-8cd5-eac86a910ede"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-3.97686176e-04-3.16334357e-05j -9.01379439e-04+1.09944888e-03j\n",
            "  -5.04900796e-05-1.59215933e-05j ... -3.49882371e-05-2.67185725e-04j\n",
            "  -3.06621338e-05+3.13011013e-05j -2.88912889e-04+8.70202255e-04j]\n",
            " [-2.37251213e-04-2.52395717e-04j -2.50101786e-04+1.20886212e-03j\n",
            "   6.75237992e-06-4.54695463e-05j ... -8.14224056e-05-2.19351461e-04j\n",
            "  -2.75699046e-05+2.62181249e-05j -5.24427476e-04+5.99017519e-04j]\n",
            " [-5.51959760e-04+1.69438731e-04j -2.04345478e-03-2.41017231e-04j\n",
            "  -7.52261685e-05+1.45501804e-05j ...  4.11820670e-06-3.89972237e-04j\n",
            "   2.07383364e-05+5.99285747e-05j -1.15621185e-03+6.51281772e-04j]\n",
            " [-3.53788441e-04-1.25095706e-04j -1.02909432e-03+8.54003487e-04j\n",
            "  -4.97961232e-05-3.53509038e-07j ...  1.45428747e-04-2.07594760e-04j\n",
            "  -4.11722616e-05+1.87965277e-06j  7.32151897e-05+8.59350840e-04j]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test vectorized version\n",
        "x_vec_tensor = torch.from_numpy(x_vec)\n",
        "xis_plus_tensor = torch.from_numpy(xis_plus)\n",
        "x_rfs_vec = input_to_rfs_torch_vectorized(x_vec_tensor, A_fun, a_fun, xis_plus_tensor, num_rand_features, dim, device='cpu', bias_term=bias)\n",
        "print(x_rfs_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjnmjV6WoMnM",
        "outputId": "17757d03-de0f-45bf-aa4a-4b477512f3d1"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-3.9769e-04-3.1633e-05j, -9.0138e-04+1.0994e-03j,\n",
            "         -5.0490e-05-1.5922e-05j,  ...,\n",
            "         -3.4988e-05-2.6719e-04j, -3.0662e-05+3.1301e-05j,\n",
            "         -2.8891e-04+8.7020e-04j],\n",
            "        [-2.3725e-04-2.5240e-04j, -2.5010e-04+1.2089e-03j,\n",
            "          6.7524e-06-4.5470e-05j,  ...,\n",
            "         -8.1422e-05-2.1935e-04j, -2.7570e-05+2.6218e-05j,\n",
            "         -5.2443e-04+5.9902e-04j],\n",
            "        [-5.5196e-04+1.6944e-04j, -2.0435e-03-2.4102e-04j,\n",
            "         -7.5226e-05+1.4550e-05j,  ...,\n",
            "          4.1182e-06-3.8997e-04j,  2.0738e-05+5.9929e-05j,\n",
            "         -1.1562e-03+6.5128e-04j],\n",
            "        [-3.5379e-04-1.2510e-04j, -1.0291e-03+8.5400e-04j,\n",
            "         -4.9796e-05-3.5351e-07j,  ...,\n",
            "          1.4543e-04-2.0759e-04j, -4.1172e-05+1.8797e-06j,\n",
            "          7.3215e-05+8.5935e-04j]], dtype=torch.complex128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.isclose(x_rfs_vec, x_rfs_concatenated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXS_-InVZyYY",
        "outputId": "dab8f831-c566-4acb-b9f4-dc41cc9a76c8"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.isclose(x_rfs_vec, x_rfs_concatenated)) # should be L * num_rand_features = 4 * 100000 = 400,000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPCTgu6oaoO1",
        "outputId": "8292d6b4-dff8-4b39-aab4-9ef6317ddb45"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # old code, for reference\n",
        "# def input_to_rfs_torch_vectorized(xw, AB_fun, ab_fun, xis, num_rfs, dim, device,\n",
        "#                                   seed=0, normalize=False, normalization_constant=None,\n",
        "#                                   orthogonal=False, proj_matrix=None):\n",
        "#     if normalize :\n",
        "#       if normalization_constant is None :\n",
        "#         xw = torch.nn.functional.normalize(xw)\n",
        "#       else :\n",
        "#         xw = normalization_constant*torch.nn.functional.normalize(xw)\n",
        "\n",
        "#     ab_coeffs = torch_apply_along_axis(ab_fun, xis, 0)\n",
        "#     AB_coeffs = torch_apply_along_axis(AB_fun, xis, 0)\n",
        "#     torch.manual_seed(seed)\n",
        "\n",
        "#     if proj_matrix is None:\n",
        "#       if device == 'cpu':\n",
        "#         if orthogonal is False :\n",
        "#           gs = torch.rand(size=(num_rfs, dim))\n",
        "#         else :\n",
        "#           gs = gaussian_orthogonal_random_matrix(num_rfs, dim, scaling = 0, device = 'cpu')\n",
        "#       else :\n",
        "#         if orthogonal is False :\n",
        "#           gs = torch.rand(size=(num_rfs, dim)).cuda()\n",
        "#         else :\n",
        "#           gs = gaussian_orthogonal_random_matrix(num_rfs, dim, scaling = 0, device = 'cuda')\n",
        "#     else :\n",
        "#       if device == 'cpu':\n",
        "#         gs = proj_matrix\n",
        "#       else :\n",
        "#          gs = proj_matrix.cuda()\n",
        "#     renorm_gs = (ab_coeffs * gs.t()).t()\n",
        "#     if len(xw.shape) == 2 :\n",
        "#       dot_products = torch.einsum('ij,jk->ik', xw, renorm_gs.t())\n",
        "#     elif len(xw.shape) == 3:\n",
        "#       dot_products = torch.einsum('bij,jk->bik', xw, renorm_gs.t())\n",
        "#     else :\n",
        "#       raise ValueError(\"Unsuported Tensor shape\")\n",
        "#     squared_xw = torch.sum(torch.mul(xw, xw), dim=-1) #do not keepdims here\n",
        "#     if len(squared_xw.shape) == 1 :\n",
        "#       correction_vector = torch.outer(squared_xw / 2, torch.mul(ab_coeffs, ab_coeffs))\n",
        "#     elif len(squared_xw.shape) == 2 :\n",
        "#       correction_vector = torch.einsum('pq, r->pqr', squared_xw, torch.mul(ab_coeffs, ab_coeffs))\n",
        "#     else :\n",
        "#       raise ValueError(\"Unsupported tensor shape of xw\")\n",
        "#     diff_vector = dot_products - correction_vector\n",
        "#     return (1.0 / math.sqrt(num_rfs)) * AB_coeffs * torch.exp(diff_vector)\n",
        "\n",
        "\n",
        "# # class NNK(nn.Module) :\n",
        "# #   def __init__(self, input_weights, A_fun, a_fun, xis, num_rfs, dim, model_device, seed=0, \\\n",
        "# #                normalize=False, normalization_constant=None, orthogonal=False, proj_matrix=None):\n",
        "# #         super().__init__()\n",
        "# #         self.input_weights = input_weights\n",
        "# #         self.A_fun = A_fun\n",
        "# #         self.a_fun = a_fun\n",
        "# #         self.xis = xis\n",
        "# #         self.num_rfs = num_rfs\n",
        "# #         self.dim = dim\n",
        "# #         self.model_device = model_device\n",
        "# #         self.seed = seed\n",
        "# #         self.normalize = normalize\n",
        "# #         self.normalization_constant = normalization_constant\n",
        "# #         self.orthogonal = orthogonal\n",
        "# #         self.proj_matrix = proj_matrix\n",
        "\n",
        "# #         self.weights = input_to_rfs_torch_vectorized(xw=self.input_weights, AB_fun=self.A_fun, ab_fun=self.a_fun, xis=self.xis, \\\n",
        "# #                                                      num_rfs=self.num_rfs, dim=self.dim, device=self.model_device, seed=self.seed,\n",
        "# #                                                      normalize=self.normalize, normalize_constant=self.normalize_constant, \\\n",
        "# #                                                      orthogonal=self.orthogonal, proj_matrix=self.proj_matrix)\n",
        "# #         self.weights = nn.Parameter(self.weights)\n",
        "# #         # TODO: ADD BIAS\n",
        "\n",
        "# #   def forward(self, x):\n",
        "# #         output_x = input_to_rfs_torch_vectorized(xw=x, AB_fun=self.A_fun, ab_fun=self.a_fun, xis=self.xis, \\\n",
        "# #                                                      num_rfs=self.num_rfs, dim=self.dim, device=self.model_device,\n",
        "# #                                                      seed=self.seed, normalize=self.normalize, \\\n",
        "# #                                                      normalize_constant=self.normalize_constant, \\\n",
        "# #                                                      orthogonal=self.orthogonal, proj_matrix=self.proj_matrix\n",
        "# #                                                     )\n",
        "# #         return output_x @ self.weights.t()"
      ],
      "metadata": {
        "id": "bTfU_-qnhcJ1"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}